--- 
title: "Speed Dating Analysis and Visualization"
author: "Amrutha Varshini Sundar, Srividya Inampudi and, Parv Joshi"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Introduction

In this project, we are using data visualization for the purpose of explanatory data analysis. The data we are using for the project is the **2006 Speed Dating Dataset** gathered by three professors of Columbia Business School: Sheena Iyengar, Emir Kamenica, Itamar Simonson. 

The dataset and the data key of the experiment can be found on the website https://data.world/annavmontoya/speed-dating-experiment. The webpage archive of the experiment conducted is https://www8.gsb.columbia.edu/researcharchive/articles/867. 

**Data Source:** https://data.world/annavmontoya/speed-dating-experiment

This data was found as part of an experiment that involved finding the major factors which affect the decision of people from both genders if they would consider someone from the opposite sex as their partner. In this experiment, every participant is allowed to meet every other participant of the opposite gender, and the rating is given based on the participant’s perception during a four-minute conversation. The link to the full article is https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/867/fisman%20iyengar.pdf.

The full citation for the data is:

Fisman, Raymond, Sheena Iyengar, Emir Kamenica, and Itamar Simonson. “Gender Differences in Mate Selection: Evidence from a Speed Dating Experiment.” *Quarterly Journal of Economics 121*, no. 2 (May 2006): 673-97.

The main conclusions from the experiment were as follows -

* Women generally give more importance to the intelligence and the race of their partner
* Men are more responsive to physical attractiveness. 
* The majority of men don’t give much value or importance to women’s intelligence or ambition when it exceeds their own.
* Women tend to prefer men who grew up in affluent neighborhoods. 
* Male selectivity is invariant to group size, while female selectivity is strongly increasing in group size.

We aim to visualize and verify these results and different patterns through various graphs, plots, and different visualization techniques. We also want to analyze and find some underlying or hidden patterns from the dataset which have not been explored yet.

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data sources

The data set used for the project is [Speed Dating Data.csv](https://github.com/amruthasundar/final-edav-project/blob/main/Speed%20Dating%20Data.csv). The key used along with this dataset can be found [here](https://github.com/amruthasundar/final-edav-project/blob/main/Speed%20Dating%20Data%20Key.doc).

This data was published in the *Quarterly Journal of Economics* in May 2006, by three Columbia University professors: Sheena Iyengar, Emir Kamenica, and Itamar Simonson. The link to their paper can be found [here](https://www0.gsb.columbia.edu/mygsb/faculty/research/pubfiles/867/fisman%20iyengar.pdf).

## The Experiment

This data was compiled by the professors as part of a speed dating experiment conducted in 21 waves and 13 days from 2002 to 2004.


The dates of the experiments are given below:

Wave 01: Oct 16, 2002

Wave 02: Oct 23, 2002

Wave 03: Nov 12, 2002

Wave 04: Nov 12, 2002

Wave 05: Nov 20, 2002

Wave 06: Mar 26, 2003

Wave 07: Mar 26, 2003

Wave 08: Apr 02, 2003

Wave 09: Apr 02, 2003

Wave 10: Sep 24, 2003

Wave 11: Sep 24, 2003

Wave 12: Oct 07, 2003

Wave 13: Oct 08, 2003

Wave 14: Oct 08, 2003

Wave 15: Feb 24, 2004

Wave 16: Feb 25, 2004

Wave 17: Feb 25, 2004

Wave 18: Apr 06, 2004

Wave 19: Apr 06, 2004

Wave 20: Apr 07, 2004

Wave 21: Apr 07, 2004


The experiment included every participant speed dating every participant from the opposite sex for four minutes, and after each 4 minute session, they were asked to fill a questionnaire of questions that asked them to rate their partner in terms of Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests. 

Moreover, they were asked if they would date them again. Demographic data of each participant was also collected for the purpose of the analysis.


## Data Description

All the data collected was given in a single CSV file with 195 columns and 8373 observations. Most of the variables given are numbers, either of the type integer of numeric, since they are mostly about ratings. However, variables such as field, from location, career, etc. were character fields.


## Issues with the data

We noticed that in different waves, the data 

For example, the ratings asked in waves 6-9 were ratings on a scale from 1 to 10. For all other waves, they were asked for a 100 point allocation instead. Some questions required distribute the 100 points among the six attributes. This made the units different for different questions. Also, some questions were introduced after the 9th wave, and hence has all values missing for waves 1-9.

Moreover, some questions did not have the option of rating for Shared Interests, while some did. This brought a mismatch in the number of columns (answers) per question. Also, some questions relating to final decisions were sensitive and hence data on those questions were never published. Hence
we had to make use of the data that was available.

<!--chapter:end:02-data.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Data transformation

Here is the process on how we transformed our data.

Firstly, since there were a lot of problems with waves up to 9, we decided to exclude it from our analysis. We found most of our results using waves 10-21. However, some results related to entire populations and hence used data for all waves.

Moreover, there are too many variables recorded in the experiment. We picked a few important ones for our analysis. We chose 12 important variables along with 3 variables which answered survey questions for each wave. The 12 important variables we chose are:

*1. iid: unique subject number, grouped by wave id gender*

*2. pid: partner’s iid number*

*3. gender:	Female = 0,	Male = 1*

*4. match: 1 = yes, 0 = no*

*5. age: age of subject*

*6. field: field of study*

*7. race: Black/African American = 1,*
	    *European/Caucasian-American = 2,*
    	*Latino/Hispanic American = 3,*
    	*Asian/Pacific Islander/Asian-American = 4,*
    	*Native American = 5,*
    	*Other = 6*
    	
*8. from: Where are you from originally (before coming to Columbia)? *

*9. income: Median household income based on zipcode*

*10. goal: What is your primary goal in participating in this event? *
	    *Seemed like a fun night out = 1,*
    	*To meet new people = 2,*
    	*To get a date = 3,*
    	*Looking for a serious relationship = 4,*
    	*To say I did it = 5,*
    	*Other = 6*

*11. career: What is your intended career?*

*12. wave: wave number*

These are the common variables we chose to analyze missing values. There were several questions in this experiment, but we chose three in specific, which were most relevant to our project and analyzed missing values for all three questions separately.

The three questions are:

**Question 1_1:**

**We want to know what you look for in the opposite sex.**

*13. attr1_1: Attractive*

*14. sinc1_1: Sincere*

*15. intel1_1: Intelligent*

*16. fun1_1: Fun*

*17. amb1_1: Ambitious*

These are the 5 other variables of question 1_1.


**Question 2_1:**

**What do you think the opposite sex looks for in a date?**

*13. att2_1: Attractive*

*14. sinc2_1: Sincere*

*15. intel2_1: Intelligent*

*16. fun2_1: Fun*

*17. amb2_1: Ambitious*

These are the 5 other variables of question 2_1.


**Question 4_1:**

**Now we want to know what you think MOST of your fellow men/women look for in the opposite sex**

*13. attr4_1: Attractive*

*14. sinc4_1: Sincere*

*15. intel4_1: Intelligent*

*16. fun4_1: Fun*

*17. amb4_1: Ambitious*

These are the 5 other variables of question 4_1.

The file with the code for the transformation is [here](https://github.com/amruthasundar/final-edav-project/blob/main/_data_transformation.R).

<!--chapter:end:03-cleaning.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
```{r libraries}
# , warning = FALSE, message = FALSE, echo = TRUE, include = FALSE
# knitr::opts_chunk$set(warning = FALSE, 
#                       message = FALSE,
#                       echo = TRUE,
#                       include = FALSE)
# Libraries
library(tidyverse)
library(patchwork)
library(repr)
library(ggnewscale)
library(RColorBrewer)
library(Lock5withR)
```

# Missing values

For the missing values chapter, we chose slightly more variables than the ones mentioned in the previous chapters. This is to represent a more general version of the missing value patterns in the data set.
 
```{r read_data}
df = read.csv("Speed Dating Data.csv")

col_common = c("iid", "pid", "gender", "match", "age", "field", "race", "from", "income", "goal", "career", "wave")
df_1 = df[c(col_common, "attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1")]
df_2 = df[c(col_common, "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1")]
df_3 = df[c(col_common, "attr3_1", "sinc3_1", "intel3_1", "fun3_1", "amb3_1")]
df_4 = df[c(col_common, "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1")]
df_5 = df[c(col_common, "attr5_1", "sinc5_1", "intel5_1", "fun5_1", "amb5_1")]
colnames(df_1) = c("iid", "pid", "gen", "mch", "age", "fld", "rce", "frm", "inc",
                   "goal", "car", "wve", "at1", "si1", "in1", "fu1", "am1")
colnames(df_2) = c("iid", "pid", "gen", "mch", "age", "fld", "rce", "frm", "inc",
                   "goal", "car", "wve", "at1", "si1", "in1", "fu1", "am1")
colnames(df_3) = c("iid", "pid", "gen", "mch", "age", "fld", "rce", "frm", "inc",
                   "goal", "car", "wve", "at1", "si1", "in1", "fu1", "am1")
colnames(df_4) = c("iid", "pid", "gen", "mch", "age", "fld", "rce", "frm", "inc",
                   "goal", "car", "wve", "at1", "si1", "in1", "fu1", "am1")
colnames(df_5) = c("iid", "pid", "gen", "mch", "age", "fld", "rce", "frm", "inc",
                   "goal", "car", "wve", "at1", "si1", "in1", "fu1", "am1")
```

## Number of missing attributes for each wave per question - Bar plots and Cleaveland Plots

First, we plotted a barplot of the maximum number of missing answers given (for variables 13 - 17) per wave for each question. This helps us understand which waves had people that had a considerable amount of missing values for each question.

```{r}
na_wave = function(df) {
  
  na_by_wave = df %>% 
  mutate(ans = rowSums(is.na(df[, c(13:17)]))) %>% 
  group_by(wve) %>% 
  summarise(na = max(ans))
  
  na_wave_plot = ggplot(na_by_wave, aes(x = reorder(wve, -na), y = na)) +
    geom_col(color = "#77D86C", fill = "#B3ECAD") +
    xlab("Wave") +
    ylab("Number of missing answers") +
    ggtitle("Missing answers by wave",
            subtitle = "Missing answers = missing values in columns 'attr', 'sinc', 'intel', 'fun', and 'amb'")
  
  return(na_wave_plot)
}
```

```{r}
na_wave_cleaveland <- function(df) {
  
  na_by_wave = df %>% 
  mutate(ans = rowSums(is.na(df[, c(13:17)]))) %>% 
  group_by(wve) %>% 
  summarise(na = max(ans))
  
  ggplot(na_by_wave, aes(x=reorder(wve, +na), y=na)) +
    xlab("WAVE") +
    ylab("Number of missing attributes")+
    geom_point(color = "#77D86C", size = 4) + 
    coord_flip() +
  theme_bw() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(colour = "grey60", linetype = "dashed")
  )
  
}
```

**For Question 1_1:**
```{r}
na_wave(df_1)
```

**For Question 1_1**
```{r}

na_wave_cleaveland(df_1)

```

We can observe that in waves 2, 3, 6, 13, and 14; there are people who did not answer any of the 5 attributes in question 1_1. In wave 5, the maximum number of missing attributes is 1, so everyone rated at least 4 of 5 attributes. In all other waves, all attributes were answered by everyone.

**For Question 2_1:**
```{r}
na_wave(df_2)
```

**For Question 2_1**
```{r}

na_wave_cleaveland(df_2)

```

Same as question 1_1, we can observe that in waves 2, 3, 6, 13, and 14; there are people who did not answer any of the 5 attributes in question 2_1. In wave 5, the maximum number of missing attributes is 1, so everyone rated at least 4 of 5 attributes. In all other waves, all attributes were answered by everyone.

**For Question 3_1:**
```{r}
na_wave(df_3)
```

**For Question 3_1**
```{r}

na_wave_cleaveland(df_3)

```

We can observe that in waves 2, 3, 6, 13, 14, 15 and 16; there are people who did not answer any of the 5 attributes in question 3_1. In all other waves, all attributes were answered by everyone. Here, the maximum number of missing answers given are all or none.

**For Question 4_1:**
```{r}
na_wave(df_4)
```

**For Question 4_1**
```{r}

na_wave_cleaveland(df_4)

```

We can observe that in waves 1, 2, 3, 4, 5, 6, 13, and 14; there are people who did not answer any of the 5 attributes in question 4_1. In all other waves, all attributes were answered by everyone. Here too, the maximum number of missing answers given are all or none.

**For Question 5_1:**
```{r}
na_wave(df_5)
```


**For Question 5_1**
```{r}

na_wave_cleaveland(df_5)

```

We can observe that in waves 1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, and 16; there are people who did not answer any of the 5 attributes in question 5_1. In all other waves, all attributes were answered by everyone. Here too, the maximum number of missing answers given are all or none.

## Heatmap

The above plots are only furnishing information regarding the count of missing values. We can drill down to check missing values by wave. Heatmaps are one way to visualize the details of missing columns. Below plots show an example of heatmaps for several questions across a single wave.

```{r}
na_wave_heat <- function(df, wv, a, b) {
  
  df_na <- df %>%
    filter(wve == wv)
  
  df_na <- df_na[a:b,] 
  
  df_na <- df_na %>%  
    rownames_to_column("id") %>%
    gather(key, value, -id) %>%
    mutate(missing = ifelse(is.na(value), "yes", "no"))
  
  na_heat <- ggplot(df_na, aes(x = key, y = fct_rev(id), fill = missing)) +
    geom_tile(color = "white") +
    xlab("Column Names") +
    ylab("id")+
    ggtitle("Missing values heatmap") +
    #scale_fill_viridis_d(option = "plasma") +
    scale_fill_manual(values=c("#77D86C", "#DD8C75")) +
    theme_bw()
  
  return(na_heat)
}
```

**For Question 1_1 and wave 5**
```{r}

na_wave_heat(df_1, 5, 150, 180)

```


**For Question 1_1 and wave 6**
```{r}

na_wave_heat(df_1, 6, 1, 30)

```


**For Question 2_1 and wave 5**
```{r}

na_wave_heat(df_2, 5, 140, 180)

```


**For Question 2_1 and wave 6**
```{r}

na_wave_heat(df_2, 6, 10,45)

```


## Missing patterns question 

Since we have 8378 observations, we used row percentages instead of row count in our missing values plot. Below are the plots for missing values per question.

```{r missing_value_pattern_function}
missing_patterns_plot = function(df, percent, subtitle) {
  
  # Finding number of rows missing for each column, and sorting them
  col_na = data.frame(count_c = colSums(is.na(df)) %>%
    sort(decreasing = TRUE))
  
  col_na_1 = as.data.frame(t(col_na))
  df = df[names(col_na_1)]
  
  # Finding the number of rows missing for each missing pattern
  row_missing_pattern = data.frame(is.na(df)) %>%
    group_by_all() %>%
    count(name = "count", sort = TRUE) %>%
    ungroup()
  
  row_missing_pattern$index = as.factor(1:nrow(row_missing_pattern))
  
  tidy_pattern = row_missing_pattern %>% 
    gather(key = "column_name", value = "missing", -index, -count)
  
  row_missing = row_missing_pattern[, -c(ncol(row_missing_pattern)-1, ncol(row_missing_pattern))]
  row_missing <- row_missing[names(col_na_1)]
  
  c_col <- c(names(row_missing))
  
  row_missing_pattern$missing = apply(row_missing, 1, any)
  
  # geom_text() position defining
  
  val = max(as.numeric(tidy_pattern$index))
  columns_ = unique(tidy_pattern$column_name)
  name = columns_[as.integer(length(columns_)/2) + 1]
  
  no_missing = as.numeric(filter(tidy_pattern %>% 
                                   group_by(index) %>% 
                                   summarize(missing=max(missing)), 
                                 missing==0)['index'])
  
  # Plotting
  
  if (percent == TRUE) {
    col_na = col_na %>% 
      mutate(count_c = 100*count_c/sum(count_c))
    
    row_missing_pattern = row_missing_pattern %>% 
      mutate(count = 100*count/sum(count))
  }
  
  col_count = ggplot(data=col_na, aes(x=unique(factor(tidy_pattern$column_name, levels = colnames(row_missing))), y=count_c)) +
    geom_bar(stat = "identity", fill = "#77D86C") +
    theme_light() +
    theme(panel.grid.minor.x = element_blank(),
          panel.grid.major.x = element_blank()) +
    xlab("") +
    ylab(ifelse(percent == TRUE, "%\nRows\nMissing", "Nums\nRows\nMissing")) +
    ggtitle("Missing Value Patterns", subtitle = subtitle)
  
  row_count = 
    ggplot(row_missing_pattern, aes(x = fct_rev(factor(index)), y = count, fill = missing)) +
    geom_bar(stat = "identity") +
    theme_light() +
    theme(panel.grid.minor.y = element_blank(),
          panel.grid.major.y = element_blank()) +
    scale_fill_manual(values = c("#77D86C", "#B3ECAD")) +
    theme(legend.position = "none") +
    xlab("") +
    ylab(ifelse(percent == TRUE, "% Rows", "Row Count")) +
    coord_flip()
  
  missing_pattern = 
    ggplot(tidy_pattern,
           aes(x = factor(column_name, levels = colnames(row_missing)), 
               y = fct_rev(index), 
               fill = missing)) +
    geom_tile(color = "white", lwd = 1.5) +
    scale_fill_brewer(palette = "Reds") +
    theme(legend.position = "none") +
    xlab("Variable") +
    ylab("Missing Pattern") +
    new_scale_color() +
    geom_tile(tidy_pattern, mapping = aes(factor(column_name), val - no_missing + 1), fill = "#1C1A1A06") +
    annotate("text", x = name, y = val - no_missing + 1, label = "Complete Cases")
  
  missing_value_plot = col_count + 
  plot_spacer() + 
  missing_pattern + 
  row_count + 
  plot_layout(heights = c(1, 2), widths = c(4, 1))
  
  return(missing_value_plot)
}
```


**For question 1_1:**

```{r}
missing_patterns_plot(df_1, percent = TRUE, subtitle = "For question 1_1")
```

We can observe that we have 9 missing value patterns in the data for question 1_1. We can see that almost above 90% of the observations come under the complete cases pattern. The second most observed pattern is that there is missing values for attractiveness, sincerity, intelligence, fun, ambitious, age, goal, and race. The third most observed pattern is that there is missing values only for age.

Moreover, we observe that about 14% of the rows have a missing value for ambitious, which is the highest percentage of missing rows for question 1_1. The second most missing column is age, with slightly lower than 14% of the rows missing it; followed by fun, with about 13% of the rows missing it.


**For question 2_1:**

```{r}
missing_patterns_plot(df_2, percent = TRUE, subtitle = "For question 2_1")
```

We can observe that we have 8 missing value patterns in the data for question 2_1. We can see that almost above 90% of the observations come under the complete cases pattern. The second most observed pattern is that there is missing values for attractiveness, sincerity, intelligence, fun, ambitious, age, goal, and race. The third most observed pattern is that there is missing values only for age.

Moreover, we observe that about 14% of the rows have a missing value for age, which is the highest percentage of missing rows for question 2_1. The second most missing column is ambitious, with about 13% of the rows missing it; followed by fun, with about 12% of the rows missing it.


**For question 3_1:**

```{r}
missing_patterns_plot(df_3, percent = TRUE, subtitle = "For question 3_1")
```


We can observe that we have 7 missing value patterns in the data for question 3_1. We can see that almost above 90% of the observations come under the complete cases pattern. The second most observed pattern is that there is missing values for attractiveness, sincerity, intelligence, fun, ambitious, age, goal, and race. The third most observed pattern is that there is missing values only for age.

Moreover, we observe that all five attributes, attractiveness, sincerity, intelligence, fun, and ambitious are missing in about 13.5% of the rows, which is the highest percentage of missing rows for question 3_1. The second most missing column is age, with about 12.5% of the rows missing it; followed by goal, with about 10% of the rows missing it.


**For question 4_1:**

```{r}
missing_patterns_plot(df_4, percent = TRUE, subtitle = "For question 4_1")
```


We can observe that we have 8 missing value patterns in the data for question 4_1. We can see that almost above 75% of the observations come under the complete cases pattern. About 20% of the observations have missing values for for all five attributes, i.e., attractiveness, sincerity, intelligence, fun, and ambitious.

The second most observed pattern is that there is missing values for all five attributes, i.e., attractiveness, sincerity, intelligence, fun, and ambitious. The third most observed pattern is that there is missing values for attractiveness, sincerity, intelligence, fun, ambitious, age, goal, and race.

Moreover, we observe that all five attributes, attractiveness, sincerity, intelligence, fun, and ambitious are missing in about 19% of the rows, which is the highest percentage of missing rows for question 4_1. The second most missing column is age, with about 1% of the rows missing it; followed by goal, with less than 1% of the rows missing it.


**For question 5_1:**

```{r}
missing_patterns_plot(df_5, percent = TRUE, subtitle = "For question 5_1")
```


We can observe that we have 8 missing value patterns in the data for question 5_1. We can see that almost 58% of the observations come under the complete cases pattern. About 20% of the observations have missing values for for all five attributes, i.e., attractiveness, sincerity, intelligence, fun, and ambitious.

The second most observed pattern is that there is missing values for all five attributes, i.e., attractiveness, sincerity, intelligence, fun, and ambitious. The third most observed pattern is that there is missing values for attractiveness, sincerity, intelligence, fun, ambitious, age, goal, and race.

Moreover, we observe that all five attributes, attractiveness, sincerity, intelligence, fun, and ambitious are missing in about 20% of the rows, which is the highest percentage of missing rows for question 5_1. The second most missing column is age, with about 1% of the rows missing it; followed by goal, with less than 1% of the rows missing it.

*We have noted only the top three of the most seen pattern and the number of missing rows per column. However, we could bot make a note for others as their percentages are too small to interpret from the plot.*

<!--chapter:end:04-missing.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Results

```{r}
df = read.csv("Speed Dating Data.csv")

library(tidyverse)
library(patchwork)
library(RColorBrewer)
library(GGally)
library(dplyr)
```

## Representation of paricipants by different categories

First, let us check the representation of our participants. This answers the question if the sample of students participating in this research experiment is truly randomized. This can help us know if there is/are a group of people that is/are over-represented. In other words, it tells us about the selection bias.

We have chosen four characteristics of the participants: field of study, age, race, and intended career. Here are the bar plots for the same.

*Note: The field, race, and career codes are summarized in the [data key](https://github.com/amruthasundar/final-edav-project/blob/main/Speed%20Dating%20Data%20Key.doc).*

```{r fig.width=12, fig.height=9}
df_by_field = df %>% 
  group_by(field_cd) %>% 
  summarise(count = n()) %>% 
  mutate(per = count*100/sum(count))

df_by_field$field_cd = as.factor(df_by_field$field_cd)

g_by_field = ggplot(df_by_field, aes(x = field_cd, y = per)) +
  geom_bar(stat = "identity", color = "#77D86C", fill = "#B3ECAD", alpha = 0.9) +
  xlab("Field of Study") +
  ylab("Percentage of participants") +
  ggtitle("Representation of participants by field of study")

df_by_age = df %>% 
  group_by(iid) %>% 
  select(iid, age)

df_by_age = unique(df_by_age)

g_by_age = ggplot(df_by_age, aes(x = age)) +
  geom_bar(color = "#77D86C", fill = "#B3ECAD", alpha = 0.9) +
  #scale_x_binned() +
  xlab("Age Group") +
  ylab("Number of participants") +
  ggtitle("Representation of participants by age")

df_by_race = df %>% 
  group_by(iid) %>% 
  select(iid, race)

df_by_race$race = as.factor(df_by_race$race)

g_by_race = ggplot(df_by_race, aes(x = race)) +
  geom_bar(color = "#77D86C", fill = "#B3ECAD", alpha = 0.9, binwidth = 5) +
  xlab("Race") +
  ylab("Number of participants") +
  ggtitle("Representation of participants by race")

df_by_career = df %>% 
  group_by(iid) %>% 
  select(iid, career_c)

df_by_career = unique(df_by_career)
df_by_career$career_c = as.factor(df_by_career$career_c)

g_by_career = ggplot(df_by_career, aes(x = career_c)) +
  geom_bar(color = "#77D86C", fill = "#B3ECAD", alpha = 0.9) +
  xlab("Intended Career") +
  ylab("Number of participants") +
  ggtitle("Representation of participants by intended career")

plot = (g_by_field | g_by_age)/(g_by_race | g_by_career)
plot



```


Here are the inferences from these plots:
  
  **1. For field of study**
  
  We can see that about 23% of the participants are from the 8th field of study, which is Business/Econ/Finance. This represents the highest percentage of students. Considering that this experiment was conducted by professors from the Columbia Business School, a lot of students from that field were expected. This is followed by fields 10 (Biological Sciences/Chemistry/Physics) and 5 (Engineering) representing the 2nd and 3rd highest percentages.

The 17th field of study, which is Architecture, constitutes about less than 1% of the participants. This is the least represented group in terms of field of study. Before that, comes field 12 (undecided), before which comes field 18 (other).

**2. For age group**
  
  We can observe that most students are between the age 20 and 30, with about 68 people with age around 27. Almost all the participants are between 18 and 39 years of age, with the exception of a few of age 42 and 55.

**3. For race**
  
  We can observe that race 2, which is European/Caucasian-American is the highest represented group here, which has about 4700 participants. Second most highest represented race is 4, i.e., Asian/Pacific Islander/Asian-American with about 2000 participants.

The count for race not specified is the least among all the races. Apart from that, the least represented race is Black/African American with about 450 participants.

**4. For intended career**
  
  Here, we can observe that the intended career of most students is career 2 and 7, i.e., Academic/Research and Banking/Consulting/Finance/Marketing/Business/CEO/Entrepreneur/Admin respectively. They constitute for about 155 and 135 participants respectively. The least represented career field is career 17, which is Architecture. Similar trend was seen in the representation by field of study.


## Number of yes matches per participant

In the original experiment, the variable `match` is the target variable. This indicates if the person was matched to the other person or not. We can use a Cleveland dot plot to show the number of yes matches each participant got. As mentioned before, we are using waves 10-21. Moreover, we faceted them by waves and color coded by the gender so we can also understand patterns among genders and within individual waves.

```{r fig.width=12, fig.height=40}
df_match_total = df %>% 
  filter(wave %in% (10:21)) %>% 
  select(iid, match, wave, gender) %>% 
  group_by(iid) %>% 
  mutate(yes = sum(match))

df_match_total = unique(df_match_total)
df_match_total$iid = as.factor(df_match_total$iid)
df_match_total$wave = as.factor(df_match_total$wave)
df_match_total$gender = factor(df_match_total$gender,
                               levels = c(0,1),
                               labels = c("female","male"))
g_match_total = ggplot(df_match_total, aes(x = yes, y = reorder(iid, yes), color = gender)) +
  geom_point(size = 3) +
  scale_x_continuous(limits = c(0, 15), breaks = seq(0, 15, 5)) +
  facet_grid(rows = vars(wave), scales = "free_y", space = "free_y") +
  scale_colour_manual(values = c("#77D86C", "#DD8C75")) +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_line(colour = "grey60", linetype = "dashed")) +
  xlab("Number of yes matches") +
  ylab("Participant ID number") +
  ggtitle("Number of yes matches per person faceted by wave",
          subtitle = "For waves 10-21 (color-coded by gender)")

g_match_total
```

Here, we can observe that the maximum number of yes matches of participants in waves 10 and 13 is always less than 5. Also, the highest number of matches for waves 20 and 18 is 3. Waves 11, 14, 15, 18 and 21 have a wide variety of matches for different people.

Moreover, all waves have participants who are not matched with other people. The number of participants across all waves who did not get matched seem to have an even number of males and females. The most matched participant is a female from wave 21 who got 14 matches.

## Analysing correlation between the chosen variables

One of the important questions we want to address is visualizing whether the features in the dataset exhibit correlation among themselves or with the target variable that tells whether the pair matched or not. This is important as it will help us come up with blatant patterns observed in features that help in formulations of simple if-then rules. 
For the purposes of this visualization, we have narrowed down comparison of ratings to a specific question that were asked prior to the event - Distribute a score of 100 among the five attributes in the order of importance that you would like to see in your potential date.
  
This question is interesting to inspect if the attributes of what people look for in their potential partners bear any correlation among themselves and also with the actual number of people they agreed to match with.

```{r fig.width=12, fig.height=40}
columns = c("iid", "age", "race", "attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1")

df_distinct = df %>% group_by(iid, age, race, attr1_1, sinc1_1, intel1_1, fun1_1, amb1_1) %>% summarise(total_matches=sum(match))

ggpairs(df_distinct[c(columns,  "total_matches")], lower=list(combo=wrap("facethist")))+
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),) +
  ggtitle("Pair plot for attributes and total number of people agreed to match")
```

Some of the interesting inferences we can make regarding the plots are,

* The distribution of most of the attributes are close to normal except for ambition with seems to be right skewed. A relatively high number of participants have given a low priority to ambition of their potential partners as a deciding factor.
* While most of the features are weakly negatively correlated almost exhibiting no correlation at all, the pair of feature sincerity, attractiveness and sincerity, intelligence are negatively correlated by the greatest amount. This tells us that people who gave a high preference to sincerity gave a low preference to intelligence and attractiveness and the other set of people weighed attractiveness and intelligence over sincerity.
* The age of the participants are mostly between 20 and 30, while one senior participant over 50 did not match with anyone, evident by the anomaly of the farthest point in the age vs total_matches plot.
* The trend that is seen in all the plots of attributes of attractiveness, sincerity, intelligence, fun and ambition is that, as the rating or the priority of certain feature increases, the number of matches goes down, for a few points. This is justifiable with the points that are to the farthest right at the bottom. This is expected as participants who have a really high expectation on one feature might not find someone suitable to meet their high expectations on that feature.
* It is interesting to note that the same plot of features vs total matches also exhibits some anomaly. There seems to be one point in all these plots that have a relatively medium level rating/priority while the number of matches is pretty high. This can be verified by the top most point to the left of the plots. There were some participants who are estimated to have based their criteria of matching on factors others that the ones that were rated on. This can be infered as they have a relatively high number of matches than their peers who matched less, given the same priority for those features.

## Change of feature perception with time

Here, our objective is to find that if a participant's perspective (ratings) on which features they are looking for in the opposite sex change with time, especially before and after the speed dating experiment. For this part, we chose one question in particular from the survey: 

**What you look for in the opposite sex?**

Now, this question was asked to the same people multiple times. We specifically choose three times. The times we chose were: 

1) Asked before the speed dating experiment

2) Asked the day after the speed dating experiment

3) Asked 3-4 weeks after the speed dating experiment

To answer our question, we decided to use parallel coordinate axis plots since it helps us to visualize multiple variables at once and also account for correlations among the variables by comparing the direction of the lines (which are our participants in this case). We color coded them by gender to find more inferences. Again, we used waves 10-21 for this plot.

We did this for each of the five features: Attractiveness, Sincerity, Intelligence, Fun, and Ambition. We normalized the values of each variable in the plots using the `uniminmax` scale. We did not use splines because we believe we lost information if we include it.

Here are the plots:

```{r fig.width=10, fig.height=10}
col_common = c("iid", "pid", "gender", "match", "age", "field", "race", "from", "income", "goal", "career", "wave")
df_a = df[c(col_common, "attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
                            "attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
                            "attr1_3", "sinc1_3", "intel1_3", "fun1_3", "amb1_3")]

df_a <- df_a[which(df_a$wave >= 10),]
df_a$iid = as.factor(df_a$iid)

df_1_a = df_a %>% 
  select(iid, attr1_1, attr1_2, attr1_3, match, gender)

df_1_a$gender = factor(df_1_a$gender, levels = c(0,1), labels = c("female","male"))
df_1_a = unique(df_1_a)

g_1_a = ggparcoord(df_1_a, columns = c(2:4), scale = "uniminmax", alphaLines = 0.75, groupColumn = "gender") +
  scale_colour_manual(values = c("#77D86C", "#DD8C75")) +
  xlab("Variables") +
  ylab("Normalized ratings/match values") +
  ggtitle("Perception of Attractiveness with time")




df_1_b = df_a %>% 
  select(iid, sinc1_1, sinc1_2, sinc1_3, match, gender)

df_1_b$gender = factor(df_1_b$gender, levels = c(0,1), labels = c("female","male"))
df_1_b = unique(df_1_b)

g_1_b = ggparcoord(df_1_b, columns = c(2:4), scale = "uniminmax", alphaLines = 0.75, groupColumn = "gender") +
  scale_colour_manual(values = c("#77D86C", "#DD8C75")) +
  xlab("Variables") +
  ylab("Normalized ratings/match values") +
  ggtitle("Perception of Sincerity with time")



df_1_c = df_a %>% 
  select(iid, intel1_1, intel1_2, intel1_3, match, gender)

df_1_c$gender = factor(df_1_c$gender, levels = c(0,1), labels = c("female","male"))
df_1_c = unique(df_1_c)

g_1_c = ggparcoord(df_1_c, columns = c(2:4), scale = "uniminmax", alphaLines = 0.75, groupColumn = "gender") +
  scale_colour_manual(values = c("#77D86C", "#DD8C75")) +
  xlab("Variables") +
  ylab("Normalized ratings/match values") +
  ggtitle("Perception of Intelligence with time")



df_1_d = df_a %>% 
  select(iid, fun1_1, fun1_2, fun1_3, match, gender)

df_1_d$gender = factor(df_1_d$gender, levels = c(0,1), labels = c("female","male"))
df_1_d = unique(df_1_d)

g_1_d = ggparcoord(df_1_d, columns = c(2:4), scale = "uniminmax", alphaLines = 0.75, groupColumn = "gender") +
  scale_colour_manual(values = c("#77D86C", "#DD8C75")) +
  xlab("Variables") +
  ylab("Normalized ratings/match values") +
  ggtitle("Perception of Fun with time")



df_1_e = df_a %>% 
  select(iid, amb1_1, amb1_2, amb1_3, match, gender)

df_1_e$gender = factor(df_1_e$gender, levels = c(0,1), labels = c("female","male"))
df_1_e = unique(df_1_e)

g_1_e = ggparcoord(df_1_e, columns = c(2:4), scale = "uniminmax", alphaLines = 0.75, groupColumn = "gender") +
  scale_colour_manual(values = c("#77D86C", "#DD8C75")) +
  xlab("Variables") +
  ylab("Normalized ratings/match values") +
  ggtitle("Perception of Ambition with time")

g_1_123 = (g_1_a + g_1_b)/(g_1_c + g_1_d)/g_1_e
g_1_123
```

Here are the interpretations from the above plot:

**1) For Attractiveness**

Since most lines go slightly upwards from `attr1_1` to `attr1_2`, we can say that people give slightly more importance to the attractiveness of the other person a day after the speed dating experiment. From `attr1_2` to `attr1_3`, most lines go upward but some go downward. Hence, in general, most people give significant importance to the attractiveness of the partner. We can observe that few people drastically bring their rating from 25% to 100%, while some people reach 0% rating in 3-4 weeks. In general, the importance of attractiveness usually keeps increasing for males more, while there is no pattern for females. However, the increase of importance for females is more drastic in females.

**2) For Sincerity**

Since most lines go upwards from `sinc1_1` to `sinc1_2`, we can say that people give more importance to the sincerity of the other person the next day. From `sinc1_2` to `sinc1_3`, most lines go downward indicating that as most time passes (line 3-4 weeks), most people give less importance to the sincerity of the partner. For females, the importance of sincerity increases the day after the experiment, but reduces drastically in 3-4 weeks. For males, their sincerity importance level remained steady with time.

**3) For Intelligence**

Most lines go upwards from `intel1_1` to `intel1_2`, some go downward, and some remain steady (especially the one with zero rating). Generally speaking, we can say that people give more intelligence to the sincerity right after the speed dating experiment. From `intel1_2` to `intel1_3`, most lines remain steady, many go downward, and some go upward indicating that as most people remain around at similar levels or remain indifferent of their partner's intelligence. For most males, the importance of intelligence increased after the experiment and remained steady afterward. For most females, it first increased too and then decreased.

**4) For Fun**

There does not seem to be a pattern from `fun1_1` to `fun1_2` or from `fun1_2` to `fun1_3` since many lines go upward and downward both times. Moreover, the variation in the range of ratings keep increasing from `fun1_1` to `fun1_3`. Here too, the ratings of people with zero rating remained steady. No pattern is observed for particular genders.

**5) For Ambition**

Since most lines go upwards from `amb1_1` to `amb1_2`, we can say that people give more importance to the ambition of the other person the next day. From `amb1_2` to `amb1_3`, most lines go downward indicating that as most time passes (line 3-4 weeks), most people give less importance to the ambition of the partner. Here too, the ratings of people with zero rating remained steady. Moreover, some drastic increase (about 60% to 100%) and decrease (60% to 0%) of the importance of ambition is observed from `amb1_1` to `amb1_3`. For females, the importance of ambition increased after the experiment, but decreased again in 3-4 weeks. For males, there seems to be an even distribution of people with increasing and decreasing importance.

<!--chapter:end:05-results.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Interactive component



<!--chapter:end:06-interactive.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Conclusion


<!--chapter:end:07-conclusion.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Results

```{r libraries}
# , warning = FALSE, message = FALSE, echo = TRUE, include = FALSE
# knitr::opts_chunk$set(warning = FALSE, 
#                       message = FALSE,
#                       echo = TRUE,
#                       include = FALSE)
# Libraries
library(tidyverse)
library(patchwork)
library(repr)
library(ggnewscale)
library(RColorBrewer)
library(Lock5withR)
library(dplyr)
```
**Question 1_1:**

**We want to know what you look for in the opposite sex.**

**Question 2_1:**

**What do you think the opposite sex looks for in a date?**

**Question 4_1:**

**Now we want to know what you think MOST of your fellow men/women look for in the opposite sex**

# Next Day

**Question 1_2:**

**We want to know what you look for in the opposite sex.**

**Question 2_2:**

**What do you think the opposite sex looks for in a date?**

**Question 4_2:**

**Now we want to know what you think MOST of your fellow men/women look for in the opposite sex**



### SRIVIDYA PART

```{r read_data}
df = read.csv("Speed Dating Data.csv")
col_common = c("iid", "pid", "gender", "match", "age", "field", "race", "from", "income", "goal", "career", "wave")
# df_1 = df[c(col_common, "attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1")]
# df_2 = df[c(col_common, "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1")]
# df_4 = df[c(col_common, "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1")]
```

# Subsetting Dataset 

```{r}

df_work <- df[c(col_common, "attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
                            "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1",
                            "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1",
                            "attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
                            "attr2_2", "sinc2_2", "intel2_2", "fun2_2", "amb2_2",
                            "attr4_2", "sinc4_2", "intel4_2", "fun4_2", "amb4_2",
                            "satis_2")]

df_work <- df_work[ which(df_work$wave >= 10),]

```

```{r}

df_pre <- df_work[c(col_common, "attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
                            "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1",
                            "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1")]

df_pre <- df_pre[ which(df_pre$wave >= 10),]

```


```{r}

df_post <- df_work[c(col_common, "attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
                            "attr2_2", "sinc2_2", "intel2_2", "fun2_2", "amb2_2",
                            "attr4_2", "sinc4_2", "intel4_2", "fun4_2", "amb4_2",
                            "satis_2")]

df_post <- df_post[ which(df_post$wave >= 10),]

```

```{r}

df_contour_yes_pre <- df_pre[c("iid","match","attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1")]

df_contour_yes_pre<- df_contour_yes_pre[ which(df_contour_yes_pre$match == 1),]

df_contour_yes_pre<- df_contour_yes_pre %>%
                      	group_by(iid,match) %>%
                      	summarise(mean_attr1_1 = mean(attr1_1),
                      	          mean_sinc1_1 = mean(sinc1_1),
                      	          mean_intel1_1 = mean(intel1_1),
                      	          mean_fun1_1 = mean(fun1_1),
                      	          mean_amb1_1 = mean(amb1_1))

# d1 <- pivot_longer(!iid, names_to = "colmn_names", values_to = "colmn_val")

df_contour_yes_pre <- pivot_longer(data = df_contour_yes_pre, cols = c("mean_attr1_1", "mean_sinc1_1", "mean_intel1_1", "mean_fun1_1", "mean_amb1_1"))

df_contour_yes_pre$value[is.na(df_contour_yes_pre$value)]<-mean(df_contour_yes_pre$value,na.rm=TRUE)

```

```{r}

df_contour_yes_post <- df_post[c("iid","match","attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2")]

df_contour_yes_post <- df_contour_yes_post[ which(df_contour_yes_post$match == 1),]

df_contour_yes_post <- df_contour_yes_post %>%
                      	group_by(iid,match) %>%
                      	summarise(mean_attr1_2 = mean(attr1_2),
                      	          mean_sinc1_2 = mean(sinc1_2),
                      	          mean_intel1_2 = mean(intel1_2),
                      	          mean_fun1_2 = mean(fun1_2),
                      	          mean_amb1_2 = mean(amb1_2))

# d1 <- pivot_longer(!iid, names_to = "colmn_names", values_to = "colmn_val")

df_contour_yes_post <- pivot_longer(data = df_contour_yes_post, cols = c("mean_attr1_2", "mean_sinc1_2", "mean_intel1_2", "mean_fun1_2", "mean_amb1_2"))

df_contour_yes_post$value[is.na(df_contour_yes_post$value)]<-mean(df_contour_yes_post$value,na.rm=TRUE)

```

```{r}

# Contour plots for match 1 = yes post

df_contour_yes_post %>% ggplot(mapping = aes(x=name, y=value)) +
  geom_point(size=2,colour = "blue") +
  #geom_jitter() +
  theme_gray(10) + 
  ylab("Rating out of 100") + 
  xlab("Atrribute") + 
  ggtitle("Post - Atrribute vs Rating for match = yes")


# Contour plots for match 1 = yes pre

df_contour_yes_pre %>% ggplot(mapping = aes(x=name, y=value)) +
  geom_point(size=2,colour = "blue") +
  #geom_jitter() +
  theme_gray(10) + 
  ylab("Rating out of 100") + 
  xlab("Atrribute") + 
  ggtitle("Pre - Atrribute vs Rating for match = yes")

```
```{r}

df_contour_no_pre <- df_pre[c("iid","match","attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1")]

df_contour_no_pre<- df_contour_no_pre[ which(df_contour_no_pre$match == 0),]

df_contour_no_pre<- df_contour_no_pre %>%
                      	group_by(iid,match) %>%
                      	summarise(mean_attr1_1 = mean(attr1_1),
                      	          mean_sinc1_1 = mean(sinc1_1),
                      	          mean_intel1_1 = mean(intel1_1),
                      	          mean_fun1_1 = mean(fun1_1),
                      	          mean_amb1_1 = mean(amb1_1))

# d1 <- pivot_longer(!iid, names_to = "colmn_names", values_to = "colmn_val")

df_contour_no_pre <- pivot_longer(data = df_contour_no_pre, cols = c("mean_attr1_1", "mean_sinc1_1", "mean_intel1_1", "mean_fun1_1", "mean_amb1_1"))

df_contour_no_pre$value[is.na(df_contour_no_pre$value)]<-mean(df_contour_no_pre$value,na.rm=TRUE)

```

```{r}

df_contour_no_post <- df_post[c("iid","match","attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2")]

df_contour_no_post <- df_contour_no_post[ which(df_contour_no_post$match == 1),]

df_contour_no_post <- df_contour_no_post %>%
                      	group_by(iid,match) %>%
                      	summarise(mean_attr1_2 = mean(attr1_2),
                      	          mean_sinc1_2 = mean(sinc1_2),
                      	          mean_intel1_2 = mean(intel1_2),
                      	          mean_fun1_2 = mean(fun1_2),
                      	          mean_amb1_2 = mean(amb1_2))

# d1 <- pivot_longer(!iid, names_to = "colmn_names", values_to = "colmn_val")

df_contour_no_post <- pivot_longer(data = df_contour_no_post, cols = c("mean_attr1_2", "mean_sinc1_2", "mean_intel1_2", "mean_fun1_2", "mean_amb1_2"))

df_contour_no_post$value[is.na(df_contour_no_post$value)]<-mean(df_contour_no_post$value,na.rm=TRUE)

```

```{r}

# Contour plots for match 0 = no post

df_contour_no_post %>% ggplot(mapping = aes(x=name, y=value)) +
  geom_point(size=2,colour = "red") +
  #geom_jitter() +
  theme_gray(10) + 
  ylab("Rating out of 100") + 
  xlab("Atrribute") + 
  ggtitle("Post - Atrribute vs Rating for match = no")


# Contour plots for match 0 = no pre

df_contour_no_pre %>% ggplot(mapping = aes(x=name, y=value)) +
  geom_point(size=2,colour = "red") +
  #geom_jitter() +
  theme_gray(10) + 
  ylab("Rating out of 100") + 
  xlab("Atrribute") + 
  ggtitle("Pre - Atrribute vs Rating for match = no")

```

```{r}

df_bar_pre <- df_pre[c("gender","attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
                                          "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1",
                                          "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1")]

for(i in 1:ncol(df_bar_pre)){
  df_bar_pre[is.na(df_bar_pre[,i]), i] <- mean(df_bar_pre[,i], na.rm = TRUE)
}


df_bar_pre <- df_bar_pre %>% 
                    group_by(gender) %>%
                      summarise(across(everything(), mean))


```



```{r}

# df_bar_pre <- df_pre[c("gender","attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
#                                           "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1",
#                                           "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1")]
# 
# for(i in 1:ncol(df_bar_pre)){
#   df_bar_pre[is.na(df_bar_pre[,i]), i] <- mean(df_bar_pre[,i], na.rm = TRUE)
# }
# 
# 
# df_bar_pre <- df_bar_pre %>% 
#                     group_by(gender) %>%
#                       summarise(across(everything(), median))


```

```{r}

df_bar_pre_f <- df_bar_pre[ which(df_bar_pre$gender == 0),]

df_bar_pre_f <- pivot_longer(data = subset (df_bar_pre_f , select = -gender), 
                             cols = c("attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
                                          "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1",
                                          "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1"))

df_bar_pre_f$Attribute <- substr(df_bar_pre_f$name, 0, 3)


df_bar_pre_plt_f <- df_bar_pre_f%>% 
  gather(Attr, plot_v, -c(Attribute, value)) %>%
    mutate(Attribute = factor(Attribute, 
                        levels = c("att", "sin", "int", "fun", "amb"))) %>%
      arrange(Attribute)

```

```{r}

df_bar_pre_m <- df_bar_pre[ which(df_bar_pre$gender == 1),]

df_bar_pre_m <- pivot_longer(data = subset (df_bar_pre_m , select = -gender), 
                             cols = c("attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1",
                                          "attr2_1", "sinc2_1", "intel2_1", "fun2_1", "amb2_1",
                                          "attr4_1", "sinc4_1", "intel4_1", "fun4_1", "amb4_1"))

df_bar_pre_m$Attribute <- substr(df_bar_pre_m$name, 0, 3)


df_bar_pre_plt_m <- df_bar_pre_m%>% 
  gather(Attr, plot_v, -c(Attribute, value)) %>%
    mutate(Attribute = factor(Attribute, 
                        levels = c("att", "sin", "int", "fun", "amb"))) %>%
      arrange(Attribute)

```


```{r}

colourCount = 15
getPalette = colorRampPalette(brewer.pal(9, "RdYlGn"))

ggplot(data = df_bar_pre_plt_f) +
  geom_bar(aes(x = Attribute,
               y = value,
               fill = plot_v,
               color = plot_v),
           stat = "identity",
           position = position_dodge()) +
  scale_fill_manual(values = getPalette(colourCount)) +
  theme(legend.title=element_blank()) + 
  ylab("Rating out of 100") + 
  xlab("Atrributes") + 
  ggtitle("Pre - Female - Attribute rating comparison")


ggplot(data = df_bar_pre_plt_m) +
  geom_bar(aes(x = Attribute,
               y = value,
               fill = plot_v,
               color = plot_v),
           stat = "identity",
           position = position_dodge()) +
  scale_fill_manual(values = getPalette(colourCount)) +
  theme(legend.title=element_blank()) + 
  ylab("Rating out of 100") + 
  xlab("Atrributes") + 
  ggtitle("Pre - Male - Attribute rating comparison")

```


```{r}

df_bar_post <- df_post[c("gender","attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
                                          "attr2_2", "sinc2_2", "intel2_2", "fun2_2", "amb2_2",
                                          "attr4_2", "sinc4_2", "intel4_2", "fun4_2", "amb4_2")]

for(i in 1:ncol(df_bar_post)){
  df_bar_post[is.na(df_bar_post[,i]), i] <- mean(df_bar_post[,i], na.rm = TRUE)
}


df_bar_post <- df_bar_post %>% 
                    group_by(gender) %>%
                      summarise(across(everything(), mean))


```


```{r}

# df_bar_post <- df_post[c("gender","attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
#                                           "attr2_2", "sinc2_2", "intel2_2", "fun2_2", "amb2_2",
#                                           "attr4_2", "sinc4_2", "intel4_2", "fun4_2", "amb4_2")]
# 
# for(i in 1:ncol(df_bar_post)){
#   df_bar_post[is.na(df_bar_post[,i]), i] <- mean(df_bar_post[,i], na.rm = TRUE)
# }
# 
# 
# df_bar_post <- df_bar_post %>% 
#                     group_by(gender) %>%
#                       summarise(across(everything(), median))


```

```{r}

df_bar_post_f <- df_bar_post[ which(df_bar_post$gender == 0),]

df_bar_post_f <- pivot_longer(data = subset (df_bar_post_f , select = -gender), 
                             cols = c("attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
                                          "attr2_2", "sinc2_2", "intel2_2", "fun2_2", "amb2_2",
                                          "attr4_2", "sinc4_2", "intel4_2", "fun4_2", "amb4_2"))

df_bar_post_f$Attribute <- substr(df_bar_post_f$name, 0, 3)


df_bar_post_plt_f <- df_bar_post_f%>% 
  gather(Attr, plot_v, -c(Attribute, value)) %>%
    mutate(Attribute = factor(Attribute, 
                        levels = c("att", "sin", "int", "fun", "amb"))) %>%
      arrange(Attribute)

```

```{r}

df_bar_post_m <- df_bar_post[ which(df_bar_post$gender == 1),]

df_bar_post_m <- pivot_longer(data = subset (df_bar_post_m , select = -gender), 
                             cols = c("attr1_2", "sinc1_2", "intel1_2", "fun1_2", "amb1_2",
                                          "attr2_2", "sinc2_2", "intel2_2", "fun2_2", "amb2_2",
                                          "attr4_2", "sinc4_2", "intel4_2", "fun4_2", "amb4_2"))

df_bar_post_m$Attribute <- substr(df_bar_post_m$name, 0, 3)


df_bar_post_plt_m <- df_bar_post_m%>% 
  gather(Attr, plot_v, -c(Attribute, value)) %>%
    mutate(Attribute = factor(Attribute, 
                        levels = c("att", "sin", "int", "fun", "amb"))) %>%
      arrange(Attribute)

```


```{r}

colourCount = 15
getPalette = colorRampPalette(brewer.pal(9, "RdYlGn"))

ggplot(data = df_bar_post_plt_f) +
  geom_bar(aes(x = Attribute,
               y = value,
               fill = plot_v,
               color = plot_v),
           stat = "identity",
           position = position_dodge()) +
  scale_fill_manual(values = getPalette(colourCount)) +
  theme(legend.title=element_blank()) + 
  ylab("Rating out of 100") + 
  xlab("Atrributes") + 
  ggtitle("Post - Female - Attribute rating comparison")


ggplot(data = df_bar_post_plt_m) +
  geom_bar(aes(x = Attribute,
               y = value,
               fill = plot_v,
               color = plot_v),
           stat = "identity",
           position = position_dodge()) +
  scale_fill_manual(values = getPalette(colourCount)) +
  theme(legend.title=element_blank()) + 
  ylab("Rating out of 100") + 
  xlab("Atrributes") + 
  ggtitle("Post - Male - Attribute rating comparison")

```

```{r}

df_satis <- df_post[c("iid","gender","match", "satis_2")]

df_satis_f <- df_satis[ which(df_satis$gender == 0),]

df_satis_f_1 <- df_satis_f %>%
              group_by(iid) %>%
                summarise(no_matches = sum(match),
                          satisfied = median(satis_2))

# ggplot(df_satis_f_1, aes(x = no_matches, y = satisfied, fill= count(group_by(c(no_matches,satisfied))))) + 
#   geom_tile()

df_satis_f_1 <- subset (df_satis_f_1 , select = -iid)

df_satis_f_1<-data.frame(table(df_satis_f_1))

df_satis_f_1 <- df_satis_f_1 %>%
              group_by(no_matches) %>%
                mutate(freqsum = sum(Freq))

df_satis_f_1$Freq<-as.factor(as.numeric(100*(df_satis_f_1$Freq/df_satis_f_1$freqsum)))
df_satis_f_1$no_matches<-as.character(df_satis_f_1$no_matches) %>% as.numeric()
df_satis_f_1$satisfied<-as.character(df_satis_f_1$satisfied) %>% as.numeric()

# plot using ggplot
ggplot(df_satis_f_1,aes(x=no_matches,y=satisfied)) +
geom_tile(aes(fill=Freq))


# mat_satis <- as.matrix(subset (df_satis_f_1 , select = -iid))
# heatmap(mat_satis)

```

```{r}

df_satis <- df_post[c("iid","gender","match", "satis_2")]

df_satis_f <- df_satis[ which(df_satis$gender == 1),]

df_satis_f_1 <- df_satis_f %>%
              group_by(iid) %>%
                summarise(no_matches = sum(match),
                          satisfied = median(satis_2))

# ggplot(df_satis_f_1, aes(x = no_matches, y = satisfied, fill= count(group_by(c(no_matches,satisfied))))) + 
#   geom_tile()

df_satis_f_1 <- subset (df_satis_f_1 , select = -iid)

df_satis_f_1<-data.frame(table(df_satis_f_1))

df_satis_f_1$Freq<-as.factor(df_satis_f_1$Freq)
df_satis_f_1$no_matches<-as.character(df_satis_f_1$no_matches) %>% as.numeric()
df_satis_f_1$satisfied<-as.character(df_satis_f_1$satisfied) %>% as.numeric()

# plot using ggplot
ggplot(df_satis_f_1,aes(x=no_matches,y=satisfied)) +
geom_tile(aes(fill=Freq)) +
geom_text(aes(label=Freq))


# mat_satis <- as.matrix(subset (df_satis_f_1 , select = -iid))
# heatmap(mat_satis)

```


<!--chapter:end:sri_results.Rmd-->

